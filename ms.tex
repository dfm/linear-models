\documentclass[modern]{rnaastex}

\pdfoutput=1

\usepackage{microtype}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% ------------------ %
% end of AASTeX mods %
% ------------------ %

% Projects:
\newcommand{\project}[1]{\textsf{#1}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\lsst}{\project{LSST}}
\newcommand{\tess}{\project{TESS}}
\newcommand{\celerite}{\project{celerite}}
\newcommand{\celeriteterm}{\emph{celerite}}
\newcommand{\emcee}{\project{emcee}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}
\newcommand{\ie}{\foreign{i.e.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\documentname}{\textsl{Note}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\alltodo}[1]{\todo{TEAM}{red}{#1}}
\newcommand{\citeme}{{\color{red}(citation needed)}}

\newcommand{\Gaussian}[3]{\ensuremath{\frac{1}{(2\pi)^\frac{#3}{2}|#2|^\frac{1}{2}}
            \exp\left[ -\frac{1}{2}#1^\top #2^{-1} #1 \right]}}

% VECTORS AND MATRICES USED IN THIS PAPER
\newcommand{\Normal}{\ensuremath{\mathcal{N}}}
\newcommand{\mA}{\ensuremath{\bvec{A}}}
\newcommand{\mC}{\ensuremath{\bvec{C}}}
\newcommand{\mS}{\ensuremath{\bvec{\Sigma}}}
\newcommand{\mL}{\ensuremath{\bvec{\Lambda}}}
\newcommand{\vw}{\ensuremath{\bvec{w}}}
\newcommand{\vy}{\ensuremath{\bvec{y}}}
\newcommand{\vt}{\ensuremath{\bvec{\theta}}}
\newcommand{\vm}{\ensuremath{\bvec{\mu}(\bvec{\theta})}}
\newcommand{\vre}{\ensuremath{\bvec{r}}}
\newcommand{\vh}{\ensuremath{\bvec{h}}}
\newcommand{\vk}{\ensuremath{\bvec{k}}}

% \shorttitle{}
% \shortauthors{}
% \submitted{Submitted to \textit{The Astrophysical Journal}}

\begin{document}\raggedbottom\sloppy\sloppypar\frenchspacing

\title{%
    Linear models for systematics and nuisances
}

\author[0000-0002-0296-3826]{Rodrigo Luger}
\affil{Department~of~Astronomy, University~of~Washington, Box 351580, Seattle, WA 98195, USA}

\author[0000-0002-9328-5652]{Daniel Foreman-Mackey}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\section{}

In many astronomical contexts (for example, stellar time-series, or
high-contrast imaging, or stellar spectroscopy), there is structured
noise caused by systematic
effects in the astronomical source, the atmosphere, the telescope, or
the detector.
Sometimes housekeeping data, and often the science data themselves,
can be used as predictors of the systematic noise.
Linear combinations of these predictors (or linear combinations of non-linear functions
of these predictors) are often used as
computationally tractable models that can capture the nuisances.
These linear models can be used to fit and subtract systematics prior to
investigation of the signals of interest, or they can be used in a
simultaneous fit of the systematics and the signals.

In this \documentname, we show that if a
Gaussian (or else extremely wide) prior is placed on the amplitudes of the linear
components, the amplitudes can be marginalized out with an operation in pure linear
algebra, that can (often) be made fast.
We illustrate this model by demonstrating the applicability of a linear model
for the non-linear systematics in \kepler\ time-series data, where the dominant
noise source for many stars is spacecraft motion and variability.

The target of many astronomical studies is the recovery of tiny astrophysical
signals living in a sea of uninteresting (but usually dominant) noise.
More often than not, evaluation of the true physical model for these nuisances
is computationally intractable and dependent on too many (unknown) parameters
to allow rigorous probabilistic inference.
A powerful alternative to physical models are flexible data-driven models that
can reliably predict the systematic trends in a computationally tractable
framework.
A linear model\footnote{In this context, when we say ``linear'' we are
referring to a broad class of models including things like polynomials of the
inputs or Fourier models.} is written as
%
\begin{align}
\eqlabel{linearmodel}
\bvec{y} = \mA \vw + \mathrm{noise}
\end{align}
%
where $\mA$ is the \emph{design matrix}, whose columns are the basis
vectors $\{\bvec{a}_j\}$, and $\vw$ is the vector of weights, one
for each basis vector.

Such models have been used to describe the systematics in astrophysical time
series data \citep{Smith:2012, Wang:2016, Luger:2016}, galaxy spectra
\citep{Tsalmantza:2012, Ness:2015}, \alltodo{ADD MORE REFERENCES}.
One issue with flexible data-driven models, however, is their tendency to
over-fit and reduce the astrophysical signal of interest.
This is generally tackled using a dimensionality reduction technique like
principal component analysis (PCA) or by applying priors/regularization to the
weights vector $\vw$.
In the following derivation, we show that, if this prior is Gaussian, then the
marginalization over $\vw$ is analytic and easy to compute.

Consider a dataset $\vy$ of $N$ measurements $y_i$ with covariance
matrix $\mC$.
In the common case of data collected with measurement error $\sigma_i$ on
individual data points but no correlation across measurements, $\mC$ is a
diagonal matrix with $\mC_{ij} = \sigma_{i}\delta_{ij}$, although in general
the off-diagonal elements capture the covariance between different
measurements. Given a mean function $\vm$  and a linear model as in
\eq{linearmodel}, the probability of the data under the model is given by a
normal distribution with mean $\vm + \mA \vw$ and variance $\mC$:
%
\begin{align}\eqlabel{likelihood}
p(\vy | \vt, \vw) &= \Normal(\vy; \vm + \mA \vw, \mC).
\end{align}
%
However, we are specifically not interested in the \emph{value} of $\vw$.
Instead, we will marginalize over it.
To perform this marginalization we must place a prior on $\vw$ that we will
assume to be Gaussian
%
\begin{align}
p(\vw) &= \Normal(\vw; 0, \mL) \nonumber
\end{align}
%
With this prior and likelihood in \eq{likelihood}, we compute the marginalize
likelihood as
%
\begin{align}
\eqlabel{integral}
p(\vy | \vt) &= \int_{-\infty}^{\infty} p(\vw) p(\vy | \vt, \vw) \dd\vw \nonumber \\
%
             &= \int_{-\infty}^{\infty} \Gaussian{\vw}{\mL}{K} \nonumber\\
             & \quad\quad\quad \times \Gaussian{(\vre - \mA \vw)}{\mC}{N}
               \dd\vw \nonumber \\
%
             &= \frac{1}{(2 \pi)^\frac{K + N}{2} |\mL|^\frac{1}{2} |\mC|^\frac{1}{2}}
                \int_{-\infty}^{\infty} \exp \left[ -\frac{1}{2} z \right] \dd\vw
\end{align}
%
where we have used $\vre = \vy - \vm$ and
%
\begin{align}
\eqlabel{z}
z &= \vw^\top \mL^{-1} \vw + \vre^\top \mC^{-1} \vre -
     2(\mA \vw)^\top \mC^{-1} \vre + (\mA \vw)^\top \mC^{-1} \mA \vw.
\end{align}
%
The integral in \eq{integral} is easier to evaluate if we
complete the square and write:
%
\begin{align}
\eqlabel{z_square}
z &= (\vw - \vh)^\top \mS^{-1} (\vw - \vh) + k,
\end{align}
%
where, by comparison with \eq{z}, it can be shown that
%
\begin{align}
\mS^{-1} &= \mL^{-1} + \mA^\top \mC^{-1} \mA \\
%
\vh &= \bvec{\Sigma}\mA^\top \mC^{-1} \vre \\
%
k &= \vre^\top \left( \mC^{-1} - \mC^{-1} \mA \mS \mA^\top \mC^{-1} \right) \vre.
\end{align}
%
We may thus write
%
\begin{align}
\eqlabel{p(y|t)ugly}
p(\vy | \vt) &= \frac{1}{(2 \pi)^\frac{K + N}{2}
                |\mL|^\frac{1}{2}
                |\mC|^\frac{1}{2}}
                \exp \left[ -\frac{1}{2}k \right]
                \int_{-\infty}^{\infty} \exp
                \left[-\frac{1}{2}(\vw - \vh)^\top \mS^{-1} (\vw - \vh)
                \right] \dd\vw.
\end{align}
%
The integral is that of a Gaussian, which evaluates to
$(2\pi)^\frac{K}{2}|\bvec{\Sigma}|^\frac{1}{2}$.
By the Matrix Determinant Lemma \citeme,
%
\begin{align}
|\mS| = {|\mS^{-1}|}^{-1} = \frac{|\mL| |\mC|}{|\mC + \mA \mL \mA^\top|}.
\end{align}
%
Additionally, by the Woodbury Identity \citeme,
%
\begin{align}
k &= \vre^\top \left( \mC + \mA \mL \mA^\top \right)^{-1} \vre.
\end{align}
%
Combining these results, the expression in \eq{p(y|t)ugly} simplifies to
\begin{align}
\eqlabel{p(y|t)exp}
p(\vy | \vt) &= \frac{1}{(2 \pi)^\frac{N}{2}
                |\mC + \mA \mL \mA^\top|^\frac{1}{2}}
                \exp \left[ -\frac{1}{2} \big( \vy - \vm \big)^\top
                            (\mC + \mA \mL \mA^\top)^{-1}
                            \big( \vy - \vm \big)
                     \right].
\end{align}
%
This is a normal distribution with mean $\vm$ and covariance
$\mC + \mA \mL \mA^\top$:
%
\begin{align}
\eqlabel{p(y|t)normal}
p(\vy | \vt) &= \Normal (\vy; \vm, \mC + \mA \mL \mA^\top).
\end{align}

This marginalized likelihood function can be numerically maximized to find the
maximum likelihood parameters $\theta^\star$ or this likelihood can be
multiplied by a prior $p(\theta)$ and used for posterior inference.
In either case, the evaluation of the model will include the effects of
marginalizing over $\bvec{w}$ in the linear model and any uncertainties in
those values will be propagated to the results.

It is often useful to compute the value of the linear model so that we can
``remove'' systematics from the data.
To derive this, we recognize that \eq{p(y|t)normal} is the likelihood of a
Gaussian Process.
This means that conditioned on the data and a choice of the parameters
$\bvec{\theta}$, the systematics will have a Gaussian distribution with mean
$\bvec{m}$ and covariance $\bvec{\Sigma}_\bvec{m}$ given by
\citep{Rasmussen:2006}
%
\begin{align}
\bvec{m} &= \mu(\bvec{\theta}) + \mA \mL \mA^\top\, \left[\mC + \mA \mL
    \mA^\top\right]^{-1}\,\left[\bvec{y} - \mu(\bvec{\theta})\right]
    \nonumber\\
\bvec{\Sigma}_\bvec{m} &= \mA \mL \mA^\top - \mA \mL \mA^\top\,
    \left[\mC + \mA \mL \mA^\top\right]^{-1} \,
    \mA^\top \mL \mA
\end{align}

\acknowledgements
It is a pleasure to thank
  Patrick Cooper (Duquesne),
  Boris Leistedt (NYU),
  Bernhard Sch\"olkopf (MPI-IS), and
  Dun Wang (NYU)
for helping us understand all of this.

\bibliography{linear}

\end{document}
